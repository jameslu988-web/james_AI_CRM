"""
å‘é‡çŸ¥è¯†åº“æœåŠ¡ - ä½¿ç”¨OpenAI Embeddingså®ç°è¯­ä¹‰æœç´¢

åŠŸèƒ½ï¼š
1. æ–‡æ¡£ä¸Šä¼ å’Œè§£æï¼ˆæ”¯æŒPDFã€Wordã€TXTç­‰ï¼‰
2. æ–‡æœ¬åˆ†å—å’Œå‘é‡åŒ–
3. å‘é‡æ£€ç´¢ï¼ˆè¯­ä¹‰æœç´¢ï¼‰
4. çŸ¥è¯†åº“ç®¡ç†
"""

import os
import json
import hashlib
import numpy as np
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import asyncio

# æ–‡æ¡£è§£æåº“
import PyPDF2
import docx
from io import BytesIO

# OpenAI
from openai import AsyncOpenAI


class VectorKnowledgeService:
    """å‘é‡çŸ¥è¯†åº“æœåŠ¡"""
    
    def __init__(self):
        # ä½¿ç”¨ç³»ç»Ÿé…ç½®çš„ AI Hub Mix API
        self.api_key = os.getenv('AIHUBMIX_API_KEY', 'sk-5dn0RF7nn31mpHNjEfC5Ca1579F447418aE48e7b0d8b18F7')
        self.client = AsyncOpenAI(
            api_key=self.api_key,
            base_url=os.getenv('AIHUBMIX_BASE_URL', 'https://aihubmix.com/v1')
        )
        self.embedding_model = "text-embedding-3-small"  # æ›´ä¾¿å®œçš„embeddingæ¨¡å‹
    
    async def create_embedding(self, text: str) -> List[float]:
        """
        åˆ›å»ºæ–‡æœ¬å‘é‡
        
        å‚æ•°:
            text: è¦å‘é‡åŒ–çš„æ–‡æœ¬
            
        è¿”å›:
            å‘é‡æ•°ç»„
        """
        try:
            response = await self.client.embeddings.create(
                input=text,
                model=self.embedding_model
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"âŒ åˆ›å»ºå‘é‡å¤±è´¥: {str(e)}")
            raise
    
    async def batch_create_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        æ‰¹é‡åˆ›å»ºæ–‡æœ¬å‘é‡
        
        å‚æ•°:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        è¿”å›:
            å‘é‡åˆ—è¡¨
        """
        try:
            response = await self.client.embeddings.create(
                input=texts,
                model=self.embedding_model
            )
            return [item.embedding for item in response.data]
        except Exception as e:
            print(f"âŒ æ‰¹é‡åˆ›å»ºå‘é‡å¤±è´¥: {str(e)}")
            raise
    
    def parse_pdf(self, file_content: bytes) -> str:
        """è§£æPDFæ–‡ä»¶"""
        try:
            pdf_file = BytesIO(file_content)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            
            return text.strip()
        except Exception as e:
            print(f"âŒ è§£æPDFå¤±è´¥: {str(e)}")
            raise
    
    def parse_docx(self, file_content: bytes) -> str:
        """è§£æWordæ–‡æ¡£"""
        try:
            doc_file = BytesIO(file_content)
            doc = docx.Document(doc_file)
            
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            
            return text.strip()
        except Exception as e:
            print(f"âŒ è§£æWordæ–‡æ¡£å¤±è´¥: {str(e)}")
            raise
    
    def parse_txt(self, file_content: bytes) -> str:
        """è§£æçº¯æ–‡æœ¬æ–‡ä»¶"""
        try:
            # å°è¯•å¤šç§ç¼–ç 
            for encoding in ['utf-8', 'gbk', 'gb2312', 'latin-1']:
                try:
                    return file_content.decode(encoding)
                except UnicodeDecodeError:
                    continue
            raise ValueError("æ— æ³•è¯†åˆ«æ–‡ä»¶ç¼–ç ")
        except Exception as e:
            print(f"âŒ è§£ææ–‡æœ¬æ–‡ä»¶å¤±è´¥: {str(e)}")
            raise
    
    def parse_document(self, file_content: bytes, filename: str) -> str:
        """
        è§£ææ–‡æ¡£
        
        å‚æ•°:
            file_content: æ–‡ä»¶å†…å®¹ï¼ˆå­—èŠ‚ï¼‰
            filename: æ–‡ä»¶å
            
        è¿”å›:
            è§£æåçš„æ–‡æœ¬
        """
        extension = filename.lower().split('.')[-1]
        
        if extension == 'pdf':
            return self.parse_pdf(file_content)
        elif extension in ['doc', 'docx']:
            return self.parse_docx(file_content)
        elif extension == 'txt':
            return self.parse_txt(file_content)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {extension}")
    
    def split_text(
        self, 
        text: str, 
        chunk_size: int = 500, 
        overlap: int = 50
    ) -> List[str]:
        """
        æ–‡æœ¬åˆ†å—
        
        å‚æ•°:
            text: åŸå§‹æ–‡æœ¬
            chunk_size: æ¯å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            overlap: é‡å å­—ç¬¦æ•°
            
        è¿”å›:
            æ–‡æœ¬å—åˆ—è¡¨
        """
        chunks = []
        start = 0
        text_length = len(text)
        
        while start < text_length:
            end = start + chunk_size
            chunk = text[start:end]
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€å—ï¼Œå°è¯•åœ¨å¥å·å¤„æˆªæ–­
            if end < text_length:
                last_period = chunk.rfind('.')
                last_newline = chunk.rfind('\n')
                last_break = max(last_period, last_newline)
                
                if last_break > chunk_size * 0.5:  # è‡³å°‘ä¿ç•™ä¸€åŠå†…å®¹
                    end = start + last_break + 1
                    chunk = text[start:end]
            
            chunks.append(chunk.strip())
            start = end - overlap
        
        return [c for c in chunks if c]  # è¿‡æ»¤ç©ºå—
    
    def calculate_file_hash(self, file_content: bytes) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼ï¼ˆç”¨äºå»é‡ï¼‰"""
        return hashlib.md5(file_content).hexdigest()
    
    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """
        è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        
        å‚æ•°:
            vec1, vec2: å‘é‡æ•°ç»„
            
        è¿”å›:
            ç›¸ä¼¼åº¦ (0-1)
        """
        vec1_arr = np.array(vec1)
        vec2_arr = np.array(vec2)
        
        dot_product = np.dot(vec1_arr, vec2_arr)
        norm1 = np.linalg.norm(vec1_arr)
        norm2 = np.linalg.norm(vec2_arr)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return float(dot_product / (norm1 * norm2))
    
    async def upload_document(
        self,
        file_content: bytes,
        filename: str,
        title: str,
        category: str = "general",
        description: str = None,
        db_session = None
    ) -> Dict:
        """
        ä¸Šä¼ æ–‡æ¡£åˆ°çŸ¥è¯†åº“
        
        å‚æ•°:
            file_content: æ–‡ä»¶å†…å®¹ï¼ˆå­—èŠ‚ï¼‰
            filename: æ–‡ä»¶å
            title: æ–‡æ¡£æ ‡é¢˜
            category: åˆ†ç±»
            description: æè¿°
            db_session: æ•°æ®åº“ä¼šè¯
            
        è¿”å›:
            æ–‡æ¡£ä¿¡æ¯
        """
        from src.crm.database import get_session, KnowledgeDocument, KnowledgeChunk
        
        if db_session is None:
            db_session = get_session()
        
        try:
            # 0. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            file_hash = self.calculate_file_hash(file_content)
            existing_doc = db_session.query(KnowledgeDocument).filter(
                KnowledgeDocument.file_hash == file_hash,
                KnowledgeDocument.is_active == True
            ).first()
            
            if existing_doc:
                raise ValueError(f"è¯¥æ–‡ä»¶å·²å­˜åœ¨äºçŸ¥è¯†åº“ä¸­ï¼š'{existing_doc.title}'ï¼ˆæ–‡ä»¶åï¼š{existing_doc.filename}ï¼‰")
            
            # 1. è§£ææ–‡æ¡£
            print(f"ğŸ“ è§£ææ–‡æ¡£: {filename}")
            text = self.parse_document(file_content, filename)
            
            # 2. æ–‡æœ¬åˆ†å—
            print(f"âœ‚ï¸ åˆ†å—æ–‡æœ¬...")
            chunks = self.split_text(text)
            print(f"âœ… ç”Ÿæˆ {len(chunks)} ä¸ªåˆ†å—")
            
            # 3. åˆ›å»ºæ–‡æ¡£è®°å½•
            document = KnowledgeDocument(
                title=title,
                filename=filename,
                category=category,
                summary=description,  # ä½¿ç”¨summaryå­—æ®µ
                file_size=len(file_content),
                file_hash=file_hash,
                chunk_count=len(chunks),
                status='completed',
                content=text[:5000],  # ä¿å­˜å‰5000å­—ç¬¦ä½œä¸ºé¢„è§ˆ
                created_at=datetime.utcnow()
            )
            db_session.add(document)
            db_session.flush()
            
            # 4. å‘é‡åŒ–æ‰€æœ‰åˆ†å—
            print(f"ğŸ§ª å‘é‡åŒ–æ–‡æœ¬...")
            chunk_texts = [chunk for chunk in chunks]
            embeddings = await self.batch_create_embeddings(chunk_texts)
            
            # 5. ä¿å­˜åˆ†å—å’Œå‘é‡
            print(f"ğŸ’¾ ä¿å­˜åˆ°æ•°æ®åº“...")
            for idx, (chunk_text, embedding) in enumerate(zip(chunks, embeddings)):
                chunk = KnowledgeChunk(
                    document_id=document.id,
                    content=chunk_text,
                    chunk_index=idx,
                    embedding=json.dumps(embedding),  # ä»¥JSONæ ¼å¼å­˜å‚¨
                    chunk_metadata=json.dumps({}),
                    token_count=len(chunk_text) // 4,  # ç²—ç•¥ä¼°è®¡
                    char_count=len(chunk_text),
                    created_at=datetime.utcnow()
                )
                db_session.add(chunk)
            
            db_session.commit()
            
            print(f"âœ… æ–‡æ¡£ä¸Šä¼ æˆåŠŸ: {title}")
            
            return {
                "id": document.id,
                "title": document.title,
                "filename": document.filename,
                "category": document.category,
                "chunk_count": len(chunks)
            }
            
        except Exception as e:
            db_session.rollback()
            print(f"âŒ æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {str(e)}")
            raise
        finally:
            if db_session:
                db_session.close()
    
    async def search_similar(
        self,
        query: str,
        limit: int = 5,
        category: Optional[str] = None,
        min_similarity: float = 0.3,  # ğŸ”¥ æ–°å¢ï¼šæœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼
        db_session = None
    ) -> List[Dict]:
        """
        å‘é‡ç›¸ä¼¼åº¦æœç´¢
        
        å‚æ•°:
            query: æŸ¥è¯¢æ–‡æœ¬
            limit: è¿”å›ç»“æœæ•°é‡
            category: çŸ¥è¯†åº“åˆ†ç±»ï¼ˆå¯é€‰ï¼‰
            min_similarity: æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.3ï¼Œè¿‡æ»¤ä½ç›¸å…³å†…å®¹ï¼‰
            db_session: æ•°æ®åº“ä¼šè¯
            
        è¿”å›:
            ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨
        """
        from src.crm.database import get_session, KnowledgeChunk, KnowledgeDocument
        
        if db_session is None:
            db_session = get_session()
        
        try:
            # 1. å°†æŸ¥è¯¢æ–‡æœ¬å‘é‡åŒ–
            query_vector = await self.create_embedding(query)
            
            # 2. ä»æ•°æ®åº“è·å–æ‰€æœ‰æ´»è·ƒçš„åˆ†å—
            if category:
                chunks = db_session.query(KnowledgeChunk).join(
                    KnowledgeDocument
                ).filter(
                    KnowledgeChunk.is_active == True,
                    KnowledgeDocument.category == category
                ).all()
            else:
                chunks = db_session.query(KnowledgeChunk).filter(
                    KnowledgeChunk.is_active == True
                ).all()
            
            # 3. è®¡ç®—æ¯ä¸ªåˆ†å—çš„ç›¸ä¼¼åº¦
            results = []
            for chunk in chunks:
                if not chunk.embedding:
                    continue
                
                try:
                    chunk_vector = json.loads(chunk.embedding)
                    similarity = self.cosine_similarity(query_vector, chunk_vector)
                    
                    # ğŸ”¥ æ–°å¢ï¼šè¿‡æ»¤ä½äºé˜ˆå€¼çš„ç»“æœ
                    if similarity < min_similarity:
                        continue
                    
                    results.append({
                        "id": chunk.id,
                        "document_id": chunk.document_id,
                        "document_title": chunk.document.title if chunk.document else "Unknown",
                        "content": chunk.content,
                        "chunk_index": chunk.chunk_index,
                        "metadata": json.loads(chunk.chunk_metadata) if chunk.chunk_metadata else {},
                        "similarity": similarity
                    })
                except Exception as e:
                    print(f"âš ï¸ è·³è¿‡æ— æ•ˆåˆ†å— {chunk.id}: {str(e)}")
                    continue
            
            # 4. æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›å‰ N ä¸ª
            results.sort(key=lambda x: x["similarity"], reverse=True)
            
            # ğŸ”¥ æ–°å¢ï¼šè®°å½•æ—¥å¿—
            if results:
                print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³çŸ¥è¯†ç‰‡æ®µï¼ˆç›¸ä¼¼åº¦ >= {min_similarity})ï¼‰")
                print(f"   æœ€é«˜ç›¸ä¼¼åº¦: {results[0]['similarity']:.2f}")
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°ç›¸ä¼¼åº¦ >= {min_similarity} çš„çŸ¥è¯†ç‰‡æ®µ")
            
            return results[:limit]
            
        except Exception as e:
            print(f"âŒ å‘é‡æœç´¢å¤±è´¥: {str(e)}")
            raise
        finally:
            if db_session:
                db_session.close()


# å…¨å±€å®ä¾‹
_vector_service = None

def get_vector_service() -> VectorKnowledgeService:
    """è·å–å‘é‡çŸ¥è¯†åº“æœåŠ¡å•ä¾‹"""
    global _vector_service
    if _vector_service is None:
        _vector_service = VectorKnowledgeService()
    return _vector_service
