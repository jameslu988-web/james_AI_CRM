"""æµé‡è·å–API - è°·æ­Œæœç´¢æŠ“å–æ½œåœ¨å®¢æˆ·"""
from fastapi import APIRouter, HTTPException, BackgroundTasks
from pydantic import BaseModel
from typing import List, Optional
import asyncio
from concurrent.futures import ThreadPoolExecutor

router = APIRouter()

# çº¿ç¨‹æ± ç”¨äºæ‰§è¡ŒåŒæ­¥çš„çˆ¬è™«ä»»åŠ¡
thread_pool = ThreadPoolExecutor(max_workers=2)

class ProspectingConfig(BaseModel):
    """æµé‡è·å–é…ç½®"""
    keywords: List[str]
    limit: int = 50
    use_proxy: bool = False
    proxy_url: Optional[str] = None

class ProspectResult(BaseModel):
    """æœç´¢ç»“æœ"""
    title: str
    url: str
    keyword: str
    snippet: str
    email: Optional[str] = None
    phone: Optional[str] = None

# å…¨å±€å˜é‡å­˜å‚¨å½“å‰ä»»åŠ¡çŠ¶æ€
current_task_status = {
    "running": False,
    "progress": 0,
    "total": 0,
    "results": [],
    "error": None
}

def run_google_scraper(keywords: List[str], limit: int, use_proxy: bool, proxy_url: Optional[str]):
    """åœ¨çº¿ç¨‹ä¸­æ‰§è¡ŒGoogleæœç´¢å¹¶å°†ç»“æœå­˜å…¥Leadè¡¨ï¼ˆç®€åŒ–ç‰ˆ - ä»…å…³é”®è¯è¿‡æ»¤ï¼‰"""
    global current_task_status
    
    try:
        current_task_status["running"] = True
        current_task_status["progress"] = 0
        current_task_status["total"] = limit
        current_task_status["results"] = []
        current_task_status["error"] = None
        current_task_status["phase"] = "æœç´¢ä¸­"
        
        from src.prospecting.google_scraper import GoogleScraper
        from src.crm.database import get_session, Lead
        from datetime import datetime
        import logging
        
        logger = logging.getLogger(__name__)
        scraper = GoogleScraper()
        
        # é…ç½®ä»£ç†
        if use_proxy and proxy_url:
            scraper.set_proxy(proxy_url)
            logger.info(f"ä½¿ç”¨ä»£ç†: {proxy_url}")
        
        # ç¬¬1æ­¥ï¼šæ‰§è¡Œæœç´¢
        logger.info(f"ğŸ” å¼€å§‹æœç´¢ï¼Œå…³é”®è¯: {keywords}, ç›®æ ‡: {limit}æ¡")
        results = scraper.find_prospects(keywords=keywords, limit=limit)
        logger.info(f"âœ… æœç´¢å®Œæˆï¼Œè·å¾— {len(results)} æ¡åŸå§‹ç»“æœ")
        
        current_task_status["phase"] = "å…³é”®è¯è¿‡æ»¤ä¸­"
        current_task_status["total_found"] = len(results)
        
        # ç¬¬2æ­¥ï¼šç®€å•å…³é”®è¯è¿‡æ»¤ï¼ˆä¸ä½¿ç”¨AIï¼‰
        db = get_session()
        leads_created = 0
        leads_skipped = 0
        leads_rejected = 0
        
        # å†…è£¤ç›¸å…³å…³é”®è¯
        underwear_keywords = [
            'underwear', 'boxer', 'brief', 'trunk', 'lingerie', 
            'å†…è£¤', 'å†…è¡£', 'boxers', 'briefs', 'trunks'
        ]
        
        for idx, result in enumerate(results):
            current_task_status["progress"] = idx + 1
            
            url = result.get('url', '')
            title = result.get('title', '')
            snippet = result.get('snippet', '')
            
            if not url:
                leads_skipped += 1
                continue
            
            try:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = db.query(Lead).filter(Lead.website == url).first()
                if existing:
                    leads_skipped += 1
                    logger.info(f"â­ï¸ è·³è¿‡é‡å¤: {url}")
                    continue
                
                # ğŸ”‘ å…³é”®è¯è¿‡æ»¤ï¼šå¿…é¡»åŒ…å«å†…è£¤ç›¸å…³å…³é”®è¯
                combined_text = f"{title} {snippet} {url}".lower()
                has_underwear = any(kw in combined_text for kw in underwear_keywords)
                
                if not has_underwear:
                    logger.info(f"âŒ æ‹’ç»ï¼ˆæ— å…³ï¼‰: {title[:50]}")
                    leads_rejected += 1
                    continue
                
                logger.info(f"âœ… é€šè¿‡è¿‡æ»¤: {title[:50]}")
                
                # ä¿å­˜çº¿ç´¢
                lead = Lead(
                    company_name=title[:200] if title else 'Unknown',
                    website=url,
                    email=None,
                    phone=None,
                    country=None,
                    industry='å†…è¡£/å†…è£¤',
                    lead_source='Googleæœç´¢+å…³é”®è¯è¿‡æ»¤',
                    lead_status='new',
                    lead_score=50,  # åŸºç¡€åˆ†
                    priority='medium',
                    notes=f"ğŸ” æœç´¢ç»“æœ:\n{snippet}\n\nâš ï¸ éœ€è¦äººå·¥éªŒè¯",
                    created_at=datetime.utcnow(),
                    updated_at=datetime.utcnow()
                )
                
                db.add(lead)
                
                try:
                    db.flush()
                    leads_created += 1
                    logger.info(f"ğŸ’¾ ä¿å­˜çº¿ç´¢: {title[:30]}")
                except Exception as db_error:
                    db.rollback()
                    if 'duplicate key' in str(db_error).lower():
                        logger.warning(f"è·³è¿‡é‡å¤ï¼ˆIDå†²çªï¼‰: {title[:30]}")
                        leads_skipped += 1
                    else:
                        logger.error(f"ä¿å­˜å¤±è´¥: {str(db_error)}")
                    continue
                
            except Exception as e:
                logger.error(f"å¤„ç†å¤±è´¥: {str(e)}")
                continue
        
        db.commit()
        logger.info(f"ğŸ‰ ä»»åŠ¡å®Œæˆ: åˆ›å»º {leads_created} æ¡, è·³è¿‡ {leads_skipped} æ¡, æ‹’ç» {leads_rejected} æ¡")
        
        current_task_status["results"] = results
        current_task_status["progress"] = len(results)
        current_task_status["leads_created"] = leads_created
        current_task_status["leads_skipped"] = leads_skipped
        current_task_status["leads_rejected"] = leads_rejected
        current_task_status["running"] = False
        current_task_status["phase"] = "å®Œæˆ"
        
        return {
            "total_found": len(results),
            "leads_created": leads_created,
            "leads_skipped": leads_skipped,
            "leads_rejected": leads_rejected,
            "conversion_rate": f"{(leads_created / len(results) * 100):.1f}%" if results else "0%"
        }
        
    except Exception as e:
        import logging
        logger = logging.getLogger(__name__)
        current_task_status["running"] = False
        current_task_status["error"] = str(e)
        logger.error(f"çˆ¬è™«ä»»åŠ¡å¤±è´¥: {str(e)}")
        raise


@router.post("/prospecting/start")
async def start_prospecting(config: ProspectingConfig, background_tasks: BackgroundTasks):
    """
    å¯åŠ¨æµé‡è·å–ä»»åŠ¡
    """
    global current_task_status
    
    if current_task_status["running"]:
        raise HTTPException(status_code=400, detail="å·²æœ‰ä»»åŠ¡æ­£åœ¨è¿è¡Œï¼Œè¯·ç­‰å¾…å®Œæˆ")
    
    # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œ
    loop = asyncio.get_event_loop()
    loop.run_in_executor(
        thread_pool,
        run_google_scraper,
        config.keywords,
        config.limit,
        config.use_proxy,
        config.proxy_url
    )
    
    return {
        "message": "æµé‡è·å–ä»»åŠ¡å·²å¯åŠ¨",
        "config": config.dict()
    }


@router.get("/prospecting/status")
async def get_prospecting_status():
    """
    è·å–å½“å‰ä»»åŠ¡çŠ¶æ€
    """
    return current_task_status


@router.get("/prospecting/results")
async def get_prospecting_results():
    """
    è·å–æœç´¢ç»“æœ
    """
    return {
        "total": len(current_task_status["results"]),
        "results": current_task_status["results"]
    }


@router.post("/prospecting/stop")
async def stop_prospecting():
    """
    åœæ­¢å½“å‰ä»»åŠ¡
    """
    global current_task_status
    # TODO: å®ç°ä»»åŠ¡åœæ­¢é€»è¾‘
    current_task_status["running"] = False
    return {"message": "ä»»åŠ¡å·²åœæ­¢"}


class ProxyTestRequest(BaseModel):
    """ä»£ç†æµ‹è¯•è¯·æ±‚"""
    proxy_url: str

class ProxyConfigRequest(BaseModel):
    """ä»£ç†é…ç½®è¯·æ±‚"""
    proxy_url: str
    enabled: bool = True

# å…¨å±€ä»£ç†é…ç½®ï¼ˆå®é™…åº”è¯¥å­˜å‚¨åœ¨æ•°æ®åº“ä¸­ï¼‰
proxy_config = {
    "proxy_url": "socks5://127.0.0.1:10808",
    "enabled": False
}

@router.get("/prospecting/proxy-config")
async def get_proxy_config():
    """
    è·å–ä»£ç†é…ç½®
    """
    return proxy_config

@router.post("/prospecting/proxy-config")
async def save_proxy_config(request: ProxyConfigRequest):
    """
    ä¿å­˜ä»£ç†é…ç½®
    """
    global proxy_config
    proxy_config["proxy_url"] = request.proxy_url
    proxy_config["enabled"] = request.enabled
    
    # TODO: å®é™…åº”è¯¥ä¿å­˜åˆ°æ•°æ®åº“ä¸­
    
    return {
        "success": True,
        "message": "ä»£ç†é…ç½®å·²ä¿å­˜",
        "config": proxy_config
    }

@router.post("/prospecting/test-proxy")
async def test_proxy(request: ProxyTestRequest):
    """
    æµ‹è¯•ä»£ç†è¿æ¥
    """
    import httpx
    
    try:
        # httpx 0.28.xç‰ˆæœ¬ä¸­ï¼Œä½¿ç”¨mountså‚æ•°é…ç½®ä»£ç†
        mounts = {
            "http://": httpx.AsyncHTTPTransport(proxy=request.proxy_url),
            "https://": httpx.AsyncHTTPTransport(proxy=request.proxy_url),
        }
        
        async with httpx.AsyncClient(mounts=mounts, timeout=10.0) as client:
            response = await client.get("https://www.google.com")
            if response.status_code == 200:
                return {
                    "success": True,
                    "message": "ä»£ç†è¿æ¥æˆåŠŸ",
                    "status_code": response.status_code
                }
            else:
                return {
                    "success": False,
                    "message": f"ä»£ç†è¿”å›çŠ¶æ€ç : {response.status_code}"
                }
    except Exception as e:
        return {
            "success": False,
            "message": f"ä»£ç†è¿æ¥å¤±è´¥: {str(e)}"
        }
